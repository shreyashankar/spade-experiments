{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/shreyashankar/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# This notebook needs to be run in the root folder.\n",
        "import pandas as pd\n",
        "from spade.candidate_gen import generate_candidate_assertions\n",
        "from spade.execute_assertions import execute_candidate_assertions\n",
        "from spade.label_results import label_responses, prepare_for_optimizer\n",
        "from spade.check_subsumes import evaluate_all_subsumes, collate_subsumption_results\n",
        "from spade.optimizer import select_functions\n",
        "# from rich import print"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 3 prompt templates\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['You are an expert Python programmer and are helping me deduplicate assertions for my LLM pipeline.\\n\\nHere is my first function:\\n{func_a_src}\\n\\nHere is my second function: {func_b_src}\\n\\nIs there some input that function `{func_b}` returns False for while function `{func_a}` returns True? If both functions contain calls to `ask_llm` where the prompts are similar, your answer should be no. Return your answer as a JSON in ```json ``` markers with keys `answer` (yes or no) and `input` (None if your answer is no).',\n",
              " 'You are an expert Python programmer and are helping me remove redundant assertions for my LLM pipeline. My pipeline prompt template is `{prompt_template}` and an example response={response}.\\n\\nHere is my first function:\\n{func_a_src}\\n\\nHere is my second function: {func_b_src}\\n\\nIs there some different response such that function `{func_b}` returns False for while function `{func_a}` returns True? If both functions contain `ask_llm` calls to check for the same thing, your answer should be no. Return your answer as a JSON within ```json ``` ticks with keys `answer` (yes or no) and `response` (\"N/A\" if your answer is no).',\n",
              " 'You are an expert Python programmer and are helping me remove redundant assertions for my LLM pipeline. My pipeline prompt template is `{prompt_template}` and an example response={response}.\\n\\nHere is my first function:\\n{func_a_src}\\n\\nHere is my second function: {func_b_src}\\n\\nDoes the first function imply or not imply the second function? In other words, is there an example such that function `{func_b}` returns False for while function `{func_a}` returns True? If both functions contain `ask_llm` calls to check for the same thing, your answer should be no (meaning the first function implies the second). Return your answer as a JSON within ```json ``` ticks with keys `answer` (yes or no) and `response` (\"N/A\" if your answer is no). Yes means the first does not imply the second, and no means the first implies the second.']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_name = \"subsumption\"\n",
        "\n",
        "\n",
        "prompt_templates = [\n",
        "    \"You are an expert Python programmer and are helping me deduplicate assertions for my LLM pipeline.\\n\\nHere is my first function:\\n{func_a_src}\\n\\nHere is my second function: {func_b_src}\\n\\nIs there some input that function `{func_b}` returns False for while function `{func_a}` returns True? If both functions contain calls to `ask_llm` where the prompts are similar, your answer should be no. Return your answer as a JSON in ```json ``` markers with keys `answer` (yes or no) and `input` (None if your answer is no).\",\n",
        "    \"You are an expert Python programmer and are helping me remove redundant assertions for my LLM pipeline. My pipeline prompt template is `{prompt_template}` and an example response={response}.\\n\\nHere is my first function:\\n{func_a_src}\\n\\nHere is my second function: {func_b_src}\\n\\nIs there some different response such that function `{func_b}` returns False for while function `{func_a}` returns True? If both functions contain `ask_llm` calls to check for the same thing, your answer should be no. Return your answer as a JSON within ```json ``` ticks with keys `answer` (yes or no) and `response` (\\\"N/A\\\" if your answer is no).\",\n",
        "    \"You are an expert Python programmer and are helping me remove redundant assertions for my LLM pipeline. My pipeline prompt template is `{prompt_template}` and an example response={response}.\\n\\nHere is my first function:\\n{func_a_src}\\n\\nHere is my second function: {func_b_src}\\n\\nDoes the first function imply or not imply the second function? In other words, is there an example such that function `{func_b}` returns False for while function `{func_a}` returns True? If both functions contain `ask_llm` calls to check for the same thing, your answer should be no (meaning the first function implies the second). Return your answer as a JSON within ```json ``` ticks with keys `answer` (yes or no) and `response` (\\\"N/A\\\" if your answer is no). Yes means the first does not imply the second, and no means the first implies the second.\"\n",
        "]\n",
        "\n",
        "print(f\"There are {len(prompt_templates)} prompt templates\")\n",
        "\n",
        "prompt_template_strings = []\n",
        "from paper_experiments.subsumption.examples import EXAMPLES\n",
        "\n",
        "EXAMPLE = EXAMPLES[0]\n",
        "\n",
        "prompt_template_strings = prompt_templates\n",
        "    \n",
        "prompt_template_strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/shreyashankar/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating assertions...\n",
            "Prompt diff: --- \n",
            "\n",
            "+++ \n",
            "\n",
            "@@ -0,0 +1,4 @@\n",
            "\n",
            "+You are an expert Python programmer and are helping me deduplicate assertions for my LLM pipeline.\n",
            "+Here is my first function:\n",
            "{func_a_src}\n",
            "\n",
            "Here is my second function: {func_b_src}\n",
            "\n",
            "Is there some input that function `{func_b}` returns False for while function `{func_a}` returns True?\n",
            "+If both functions contain calls to `ask_llm` where the prompts are similar, your answer should be no.\n",
            "+Return your answer as a JSON in ```json ``` markers with keys `answer` (yes or no) and `input` (None if your answer is no).\n",
            "Generating assertions...\n",
            "Prompt diff: --- \n",
            "\n",
            "+++ \n",
            "\n",
            "@@ -1,4 +1,5 @@\n",
            "\n",
            "-You are an expert Python programmer and are helping me deduplicate assertions for my LLM pipeline.\n",
            "-Here is my first function:\n",
            "{func_a_src}\n",
            "\n",
            "Here is my second function: {func_b_src}\n",
            "\n",
            "Is there some input that function `{func_b}` returns False for while function `{func_a}` returns True?\n",
            "-If both functions contain calls to `ask_llm` where the prompts are similar, your answer should be no.\n",
            "-Return your answer as a JSON in ```json ``` markers with keys `answer` (yes or no) and `input` (None if your answer is no).\n",
            "+You are an expert Python programmer and are helping me remove redundant assertions for my LLM pipeline.\n",
            "+My pipeline prompt template is `{prompt_template}` and an example response={response}.\n",
            "+Here is my first function:\n",
            "{func_a_src}\n",
            "\n",
            "Here is my second function: {func_b_src}\n",
            "\n",
            "Is there some different response such that function `{func_b}` returns False for while function `{func_a}` returns True?\n",
            "+If both functions contain `ask_llm` calls to check for the same thing, your answer should be no.\n",
            "+Return your answer as a JSON within ```json ``` ticks with keys `answer` (yes or no) and `response` (\"N/A\" if your answer is no).\n",
            "Generating assertions...\n",
            "Prompt diff: --- \n",
            "\n",
            "+++ \n",
            "\n",
            "@@ -1,5 +1,7 @@\n",
            "\n",
            " You are an expert Python programmer and are helping me remove redundant assertions for my LLM pipeline.\n",
            " My pipeline prompt template is `{prompt_template}` and an example response={response}.\n",
            "-Here is my first function:\n",
            "{func_a_src}\n",
            "\n",
            "Here is my second function: {func_b_src}\n",
            "\n",
            "Is there some different response such that function `{func_b}` returns False for while function `{func_a}` returns True?\n",
            "-If both functions contain `ask_llm` calls to check for the same thing, your answer should be no.\n",
            "+Here is my first function:\n",
            "{func_a_src}\n",
            "\n",
            "Here is my second function: {func_b_src}\n",
            "\n",
            "Does the first function imply or not imply the second function?\n",
            "+In other words, is there an example such that function `{func_b}` returns False for while function `{func_a}` returns True?\n",
            "+If both functions contain `ask_llm` calls to check for the same thing, your answer should be no (meaning the first function implies the second).\n",
            " Return your answer as a JSON within ```json ``` ticks with keys `answer` (yes or no) and `response` (\"N/A\" if your answer is no).\n",
            "+Yes means the first does not imply the second, and no means the first implies the second.\n"
          ]
        }
      ],
      "source": [
        "assertions = await generate_candidate_assertions(prompt_template_strings, EXAMPLE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are an expert Python programmer and are helping me remove redundant assertions for my LLM pipeline. My pipeline prompt template is `{prompt_template}` and an example response={response}.\n",
            "\n",
            "Here is my first function:\n",
            "{func_a_src}\n",
            "\n",
            "Here is my second function: {func_b_src}\n",
            "\n",
            "Does the first function imply or not imply the second function? In other words, is there an example such that function `{func_b}` returns False for while function `{func_a}` returns True? If both functions contain `ask_llm` calls to check for the same thing, your answer should be no (meaning the first function implies the second). Return your answer as a JSON within ```json ``` ticks with keys `answer` (yes or no) and `response` (\"N/A\" if your answer is no). Yes means the first does not imply the second, and no means the first implies the second.\n"
          ]
        }
      ],
      "source": [
        "# Print last prompt template\n",
        "print(prompt_template_strings[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "959d9287de8ef84c52fe8533f29612c8fc3c2a320f5818ee50bcf28195864f47\n",
            "f470233bc39f1e72ad15b22cc71229d332e230fa8391b5023ca2ac097324437f\n",
            "Found cached results\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n",
            "Sleeping for 2 seconds\n"
          ]
        }
      ],
      "source": [
        "from paper_experiments.subsumption.candidate_assertions import ALL_FUNCTIONS\n",
        "from paper_experiments.subsumption.examples import EXAMPLES\n",
        "\n",
        "\n",
        "res = await execute_candidate_assertions(dataset_name, prompt_template_strings[-1], EXAMPLES, ALL_FUNCTIONS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4 good functions: ['assert_no_direct_code_in_response', 'assert_func_b_func_a_disagreement_check', 'assert_no_ask_llm_calls', 'assert_correct_implication_relationship']\n",
            "0.7014925373134329\n",
            "Num candidate functions: 20\n",
            "Num good examples: 47\n",
            "Num bad examples: 20\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Get parent directory of file\n",
        "# parent_dir = os.getcwd()\n",
        "# label_df = pd.read_csv(os.path.join(parent_dir, \"labeled_data.csv\"))\n",
        "\n",
        "label_df = label_responses(res, 0.75)\n",
        "\n",
        "print(label_df[\"label\"].mean())\n",
        "\n",
        "# Print stats\n",
        "print(f\"Num candidate functions: {len(ALL_FUNCTIONS)}\")\n",
        "print(f\"Num good examples: {len(label_df[label_df['label'] == 1])}\")\n",
        "print(f\"Num bad examples: {len(label_df[label_df['label'] == 0])}\")\n",
        "\n",
        "optimizer_dict = prepare_for_optimizer(res, label_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from paper_experiments.subsumption.labeled_subsumption_pairs import LABELED_SUBSUMPTION_PAIRS\n",
        "\n",
        "K = np.zeros((len(optimizer_dict[\"cost\"]), len(optimizer_dict[\"cost\"])))\n",
        "\n",
        "for a, b in LABELED_SUBSUMPTION_PAIRS:\n",
        "    K[optimizer_dict[\"func_order\"][a], optimizer_dict[\"func_order\"][b]] = 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19.0"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "K.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pickle all the results\n",
        "import inspect\n",
        "\n",
        "path_name = f\"/Users/shreyashankar/Documents/projects/promptdelta/paper_experiments/{dataset_name}/optimizer_input.pkl\"\n",
        "\n",
        "optimizer_dict[\"K\"] = K\n",
        "optimizer_dict[\"spade_functions\"] = {func.__name__: inspect.getsource(func) for func in ALL_FUNCTIONS}\n",
        "import pickle\n",
        "\n",
        "with open(path_name, \"wb\") as f:\n",
        "    pickle.dump(optimizer_dict, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 9 subsumptions out of 380 pairs.\n"
          ]
        }
      ],
      "source": [
        "subsumption_df = collate_subsumption_results(optimizer_dict[\"M\"], ALL_FUNCTIONS, optimizer_dict[\"func_order\"], optimizer_dict[\"K\"])\n",
        "subsumption_df.to_csv(f\"/Users/shreyashankar/Documents/projects/promptdelta/paper_experiments/{dataset_name}/subsumption_results.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the CBC MILP Solver \n",
            "Version: 2.10.3 \n",
            "Build Date: Dec 15 2019 \n",
            "\n",
            "command line - /Users/shreyashankar/miniforge3/envs/promptdelta/lib/python3.10/site-packages/pulp/solverdir/cbc/osx/64/cbc /var/folders/nq/ldkhrrws0xb9whw7b6rpzhc00000gn/T/087f1edea12b4aff952bad8bd968ea6b-pulp.mps timeMode elapsed branch printingOptions all solution /var/folders/nq/ldkhrrws0xb9whw7b6rpzhc00000gn/T/087f1edea12b4aff952bad8bd968ea6b-pulp.sol (default strategy 1)\n",
            "At line 2 NAME          MODEL\n",
            "At line 3 ROWS\n",
            "At line 2707 COLUMNS\n",
            "At line 10497 RHS\n",
            "At line 13200 BOUNDS\n",
            "At line 14648 ENDATA\n",
            "Problem MODEL has 2702 rows, 1447 columns and 4875 elements\n",
            "Coin0008I MODEL read with 0 errors\n",
            "Option for timeMode changed from cpu to elapsed\n",
            "Continuous objective value is 0.6 - 0.01 seconds\n",
            "Cgl0002I 572 variables fixed\n",
            "Cgl0003I 20 fixed, 0 tightened bounds, 0 strengthened rows, 0 substitutions\n",
            "Cgl0003I 1 fixed, 0 tightened bounds, 0 strengthened rows, 0 substitutions\n",
            "Cgl0004I processed model has 0 rows, 0 columns (0 integer (0 of which binary)) and 0 elements\n",
            "Cbc3007W No integer variables - nothing to do\n",
            "Cuts at root node changed objective from 1 to -1.79769e+308\n",
            "Probing was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
            "Gomory was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
            "Knapsack was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
            "Clique was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
            "MixedIntegerRounding2 was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
            "FlowCover was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
            "TwoMirCuts was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
            "ZeroHalf was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
            "\n",
            "Result - Optimal solution found\n",
            "\n",
            "Objective value:                1.00000000\n",
            "Enumerated nodes:               0\n",
            "Total iterations:               1\n",
            "Time (CPU seconds):             0.02\n",
            "Time (Wallclock seconds):       0.02\n",
            "\n",
            "Option for printingOptions changed from normal to all\n",
            "Total time (CPU seconds):       0.02   (Wallclock seconds):       0.03\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Solution Found:\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Solution Found:\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Selected Functions: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Selected Functions: \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the CBC MILP Solver \n",
            "Version: 2.10.3 \n",
            "Build Date: Dec 15 2019 \n",
            "\n",
            "command line - /Users/shreyashankar/miniforge3/envs/promptdelta/lib/python3.10/site-packages/pulp/solverdir/cbc/osx/64/cbc /var/folders/nq/ldkhrrws0xb9whw7b6rpzhc00000gn/T/3dea121716ba444e8047a7b054b436ea-pulp.mps timeMode elapsed branch printingOptions all solution /var/folders/nq/ldkhrrws0xb9whw7b6rpzhc00000gn/T/3dea121716ba444e8047a7b054b436ea-pulp.sol (default strategy 1)\n",
            "At line 2 NAME          MODEL\n",
            "At line 3 ROWS\n",
            "At line 4727 COLUMNS\n",
            "At line 18596 RHS\n",
            "At line 23319 BOUNDS\n",
            "At line 25607 ENDATA\n",
            "Problem MODEL has 4722 rows, 2287 columns and 9274 elements\n",
            "Coin0008I MODEL read with 0 errors\n",
            "Option for timeMode changed from cpu to elapsed\n",
            "Continuous objective value is 8 - 0.01 seconds\n",
            "Cgl0002I 984 variables fixed\n",
            "Cgl0003I 356 fixed, 0 tightened bounds, 286 strengthened rows, 909 substitutions\n",
            "Cgl0003I 0 fixed, 0 tightened bounds, 2 strengthened rows, 177 substitutions\n",
            "Cgl0004I processed model has 21 rows, 9 columns (9 integer (9 of which binary)) and 54 elements\n",
            "Cutoff increment increased from 1e-05 to 0.9999\n",
            "Cbc0038I Initial state - 0 integers unsatisfied sum - 0\n",
            "Cbc0038I Solution found of 15\n",
            "Cbc0038I Before mini branch and bound, 9 integers at bound fixed and 0 continuous\n",
            "Cbc0038I Mini branch and bound did not improve solution (0.07 seconds)\n",
            "Cbc0038I After 0.07 seconds - Feasibility pump exiting with objective of 15 - took 0.00 seconds\n",
            "Cbc0012I Integer solution of 15 found by feasibility pump after 0 iterations and 0 nodes (0.07 seconds)\n",
            "Cbc0001I Search completed - best objective 15, took 0 iterations and 0 nodes (0.07 seconds)\n",
            "Cbc0035I Maximum depth 0, 0 variables fixed on reduced cost\n",
            "Cuts at root node changed objective from 15 to 15\n",
            "Probing was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
            "Gomory was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
            "Knapsack was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
            "Clique was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
            "MixedIntegerRounding2 was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
            "FlowCover was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
            "TwoMirCuts was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
            "ZeroHalf was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
            "\n",
            "Result - Optimal solution found\n",
            "\n",
            "Objective value:                15.00000000\n",
            "Enumerated nodes:               0\n",
            "Total iterations:               0\n",
            "Time (CPU seconds):             0.07\n",
            "Time (Wallclock seconds):       0.07\n",
            "\n",
            "Option for printingOptions changed from normal to all\n",
            "Total time (CPU seconds):       0.08   (Wallclock seconds):       0.09\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Solution Found:\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Solution Found:\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Selected Functions: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Selected Functions: \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m11\u001b[0m, \u001b[1;36m12\u001b[0m\u001b[1m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Solve the optimization problem\n",
        "\n",
        "optimizer_res = select_functions(path_name, tau=0.25, alpha=0.6)\n",
        "\n",
        "# Pickle the results\n",
        "import pickle\n",
        "with open(f\"/Users/shreyashankar/Documents/projects/promptdelta/paper_experiments/{dataset_name}/optimizer_results.pkl\", \"wb\") as f:\n",
        "    pickle.dump(optimizer_res, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       method  ffr  example failure coverage  frac_functions_selected  \\\n",
            "0  spade_base  0.0                       0.8                     0.20   \n",
            "1   spade_cov  0.0                       0.7                     0.05   \n",
            "2   spade_sub  0.0                       0.8                     0.20   \n",
            "\n",
            "   frac_non_subsumed_excluded_functions  \n",
            "0                                  0.00  \n",
            "1                                  0.15  \n",
            "2                                  0.00  \n"
          ]
        }
      ],
      "source": [
        "# Visualize performance\n",
        "\n",
        "# Turn into DF\n",
        "import pandas as pd\n",
        "\n",
        "optimizer_res_df = []\n",
        "for method in [\"spade_base\", \"spade_cov\", \"spade_sub\"]:\n",
        "    optimizer_res_df.append({\"method\": method, \"ffr\": optimizer_res[method][\"ffr\"], \"example failure coverage\": optimizer_res[method][\"coverage\"], \"frac_functions_selected\": optimizer_res[method][\"frac_functions_selected\"], \"frac_non_subsumed_excluded_functions\": optimizer_res[method][\"frac_non_subsumed_excluded_functions\"]})\n",
        "    \n",
        "optimizer_res_df = pd.DataFrame(optimizer_res_df)\n",
        "print(optimizer_res_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "promptdelta",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
